# KVSの基本

## KVSとは何をするためのもの？

Key-Value形式のデータを扱うデータストア。
Keyを指定すると、対応するValueが呼び出される仕組み。

## KVSが生まれた背景

### Google Bigtable

Googleの代表的なKVSであるBigtableは大規模にスケール可能な分散ストレージシステムとして
開発された。

検索やGoogle Earth、Financeなど数のサービスにまたがって、様々な種類のデータ（画像、文字列 e.t.c）を
ペタバイトオーダーで管理している。

大規模なデータサイズに加え、非常に低いレイテンシ（リアルタイムストリーミング）を可能にしている。

ポイント：データの水平スケールのしやすさと読み書きのレイテンシ。ハイパフォーマンス・拡張性・可用性。

### Amazon Dynamo

Relational DBの強みは柔軟な検索クエリと強力なACID特性、関連付け可能なテーブル構造、そしてテーブル同士をJOINすることができる点である。

しかし現実は、約70%のトランザクションはプライマリーキーに基づいた単純な操作であり、残りの約20%がJOINを使わない操作であった。

したがって、RDBの強みをほとんど使っていないという現状であった。加えて、RDSは水平スケールできないという弱みがあった。スケールしたい時期に
おいて、この課題は非常に急務となっていた。

## RDSとKVSの比較

- KVSの方が分散化のコストが小さい。KVSはデータ構造が単純なため、分散化しやすい。
- KVSの方が負荷分散のコストが小さい。RDSを負荷分散するためには、DBのレプリケーションやシャード分割によるクラスタ構築やキャッシュの導入が必要になる。
結果的にコストが高くなりやすい。
- KVSの方が高可用性である。なぜだ？言語化できない。レプリカの作成が用意だから？クラスタ化するのが簡単だから？。
- KVSはRDSほどデータの堅牢性・一貫性を保つことができない。
- KVSはRDSほどトランザクションを厳密に管理できない。厳密に管理しようとすると、書き込みのパフォーマンスが落ちる。
- KVSは複雑な検索はできない。基本的にKeyをもとに探すだけ。

## 分散システムのトレードオフ

CAP定理：Availability・分断耐性・一貫性の3つを同時に満たすことはできない。

ノード間のデータ複製において、同時に次の3つの保証を提供することはできない。
分散システムのトレードオフを判断するために利用している。

### 一貫性 (Consistency)
すべてのデータ読み込みにおいて、最新の書き込みデータもしくはエラーのどちらか
を受け取る。
レプリケーションする際に問題になる。

e.g. データの更新中に読み取りが発生すると、最新のデータを返すことができない。

### 可用性 (Availability)
ノード障害により生存ノードの機能性は損なわれない。つまり、ダウンしていない
ノードが常に応答を返す。単一障害点が存在しないことが必要。

ノードがクラスタ化されている。

### 分断耐性 (Partition-tolerance)

システムは任意の通信障害などによるメッセージ損失に対し、継続して動作を行う。
通信可能なサーバーが複数のグループに分断されるケース（ネットワーク分断）を指
し、1つのハブに全てのサーバーがつながっている場合は、これは発生しない。
ただし、そのようなネットワーク設計は単一障害点をもつことになり、可用性が成
立しない。RDBではそもそもデータベースを分割しないので、このような障害とは無
縁である。

分断とは分散システム内の通信障害のこと。2つのノード間の接続が失われたか一時的に遅延している状態。

## CAP定理の解釈見直し

CAP定理の提唱者自身が「現代にそのままCAP定理をそのまま適用するのは危険」とコメント。

- インターネット上で提供する広域サービスにおいて、複数サーバーを用いた負荷分散は必須であることから分断耐性は選択せざるを得ない。
- ネットワーク上で分断が発生した場合に、一貫性を優先させるか、可用性を優先させるかの選択になる。
- 一貫性・可用性にも度合いがあり(0 or 1ではない)、システムがモードを切り替えながら稼働するケースもある。

## どうやって一貫性を担保する？

更新の際にデータをロックすることで一貫性を担保できる。
しかし、可用性とのトレードオフとなるため要件次第で決まる。

強整合性の場合、データの更新の際にデータベースをロックすることによってデータの
一貫性を担保するが、ロックされる期間が長いほどその間のデータベースアクセスがブロック
されるため可用性を犠牲にすることになる。

結果整合性はデータの更新でデータベースがロックされることはないため、可用性
とスケーラビリティを維持することができる。その代わりノード間でのデータの一貫性
はデータ複製にかかる時間に依存するため、必ずしも担保されない。

## DynamoDB

2021年の66時間にわたるAmaazon Primie Dayのピーク中に8920万request/secを処理していた。

### 可用性

DynamoDBでは同じAWSリージョン内の3つの施設間で自動的にレプリケートされる。

障害発生時には自動的に再レプリケートされる。

source: https://pages.awscloud.com/rs/112-TZM-766/images/20211022_DevAxConnect_NoSQL.pdf

### 一貫性

2種類の方法を提供しており、ケースによって使い分ける。トランザクション処理のパフォーマンスとのトレードオフ。

#### 結果整合性

書き込みは2つのノードに対して行われる。読み取りは1つのノードに対して行われる。

1/3の確率で古いデータが参照される。

#### 強い整合性

書き込みは2つのノードに対して行われる。読み取りは2つのノードに対して行われる。読み取り結果が等しい場合、どちらかの値を返す。異なる場合、残りのひとつも呼び出して多数決で多い方の値を返す。

より詳しい話：https://qiita.com/everpeace/items/632831371da5ff215995

### 柔軟なデータ構造のサポート

スキーマレス。

### スケーラビリティ

#### パーティション

パーティショニングとはDB内の大きなテーブルを複数の小さなテーブルに分割する技術です。これによりデータアクセスのパフォーマンスが向上し、データの管理が良いになります。
パーティショニングには以下のような種類があります。データアクセスのパフォーマンスとは、読み取り時の捜査量が少なくなるという点。

パーティションは単一の物理マシン内で行われる。

- Range Partitioning: データを特定の範囲に基づいて分割します。e.g. 日付範囲やID範囲など。
- List Partitioning: データを特定のリストに基づいて分割する。例えば国や地域コードなど。
- Hash Partitioning: データをハッシュ関数を使用して均等に分割する。これによりデータ量の偏りやアクセスをより均等に分散できる。

#### シャーディング

パーティションとベースは同じだが、異なる物理マシンで行う。読み取り書き込みの両方を分散処理可能にし、よりスケーラビリティが高い。

#### 分割する際の課題

何に基づいて分割するかが問題。下手な分割をすると、アクセスが集中しやすいノード（ホットスポット）が生じたり、データ量の偏りが生じる。
また、ノードを追加、削除した時の再分配の管理コストも高くつきやすい。

代表的な手法としてConsistent Hashingが挙げられる。

## Consistent Hashingをデザインしよう。

url: https://systemdesign.one/consistent-hashing-explained/

### Consistent Hashingの流れ

1. ハッシュ関数の出力は仮想リング（Hashリング）上の位置となる。
2. ハッシュ化されたノードのIPアドレスがノードの位置を決めるのに用いられる。
3. データオブジェクトのキーに対しても同様のHash関数が適用される。それを用いてパーティションの割当先のノードが決定する。

### 機能要件

- キャッシュサーバーを水平スケールさせるためのアルゴリズム。
- ホットスポットの発生を最小限に抑える。
- インターネットスケールの負荷を裁くことができる。
- アルゴリズムは既存のプロトコルで実装できる。

### 非機能要件

- スケーラビリティ
- 高可用性
- 小さいレイテンシ
- 信頼性

### Intoroduction

基本的なキャッシュを用いたサーバー構成

- クライアント
- オリジンサーバー（Cacheミスの発生）
- キャッシュサーバー

キャッシュサーバーの可用性をあげるためには、レプリケーションを行う。

レプリケーションを行うと、可用性は向上するものの、以下のトレードオフが発生している。

- 水平方向のスケーラビリティが小さい。
- レプリカ同士のデータの一貫性を担保するコストが高くつく。

まず水平方向にスケール可能とするためには、データを異なるキャッシュサーバー（nodes）に分割して保存する必要がある（シャーディング）。

それぞれのノードごとにレプリケーションを行うことで可用性を高めることができる。

### Partitioning

- Random assignment
- Single global cache
- Key range partitioning
- Static hash partitioning
- Consistent Hashing

Static hash partitioningの例として、ハッシュキーで生成した任意の数字に対してノード数で割った際の余りの値にノードを割り当てる方法がある。

Static hash partitioninigは水平スケールが難しい。ノードの削除・追加があった際にデータの割り振りが全てのデータに対して発生してしまうため。

#### Consistent hashing

##### 基礎

ノードの追加・削除が発生した際のデータの再配置を最小化できるアルゴリズム。分散システムでよく用いられるアルゴリズムである。

あらかじめ、十分に大きな数を（最大配置可能数）Kを決定する。

MD5などを用いてノードのIPアドレスなどをハッシュ関数を用いて変換する。その結果をKで割った余りがHash ring上のノードの位置となる。

データに対しても同じハッシュ関数を適用し、HAsh ring上の位置を計算する。時計回りで最も近いノードがそのデータの保管場所となる。

ノードが追加されたり、削除されたりするとデータの移動は関連ノードの周りだけで行われる。

ノード当たりの保管されるデータの平均個数は、K/N。（Nはノード数）

##### ノードの再配置による問題は？

ノードの配置の偏りによって、データの偏りが生じてしまう。

ノードの数を変えず、この問題を解決するためにノードがおける位置を一箇所ではなく複数箇所におけるようにする（仮想ノード）が用いられる。

##### データの挿入はどうする？

ハッシュリング上のノードの位置はself-balancing search treeで保管される。
データの挿入は以下の流れで行われる。

- データオブジェクトのキーのハッシュを計算する。
- ハッシュ値よりも大きいノードの位置を探索する。
- ノードにデータを挿入する。

ノード位置の探索はO(logN)回で行うことができる。

##### ノードの追加・削除をどうする？

- 新しいノードのハッシュを計算し、BSTから削除する。
- 継承元のノードのKeyから、新しいノードに移すべきキーを洗い出す。
- データを新しいノードに移す。

##### それぞれの操作の回数オーダーは？

- Add/Remove node: O(K/N + logN)
- Add/Remove key: O(logN)

#### BSTに対する操作（トランザクション）はどのように処理する？

BSTに対して、読み取りや変更（追加・削除）が複数回発生した場合にどうすべきか？

- 読み取りは複数のスレッドから同時に可能。
- 書き込み中は読み取りや他の書き込みのロックが必要。

#### 最適なハッシュ関数は？

早くてユニークなキーを生成できる要件。

MD5かな。

#### consistent hashingのメリット。

- 水平スケール可能。
- データの移動を最小限にできる。
- レプリケーションやデータのパーティションを素早く行うことができる。

仮想ノードを用いることで以下のメリットも享受できる。

- ダウンタイム中に、他の利用可能なノードを用いてロードバランスすることができる。
- 新しく追加されるノードが保有するデータを最適に分配可能。
- ノードの負荷分散を適切に行うことができる。

#### consistent hashingのデメリット

- ホットスポットが発生すると、カスケード障害が発生しやすい。
- ノードの配置に制限がない。
- ノード性能の不均一さを忘れがち？

仮想ノードを用いる際のデメリット

- 特定のデータが以上にアクセス増加しても、consistent hashingはそのデータをもつノードにリクエストを送り続けてしまう。
- キャパシティ予測・計画が複雑化する。
- BSTの管理コストなどが増大する。
- 仮想ノードと物理ノードでのレプリケーション作成ロジックが増えてしまう。
- 仮想ノードが落ちると、被害は大きくなりがち。

# KVSをデザインしよう

## KVSの機能要件は？

## KVSの非機能要件は？

# 分散KVSをデザインしよう
